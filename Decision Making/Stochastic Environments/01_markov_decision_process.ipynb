{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [previous notebook](https://github.com/Kiboya/Artificial-Intelligence-Notebooks/blob/2b0545ce5d1aceb5c2ae1f6f26348f565fef1504/Decision%20Making/Deterministic%20Environments/01_pathfinding_strategies.ipynb), we explored various pathfinding algorithms such as BFS, DFS, Dijkstra, and A*. These algorithms are effective for determining the shortest path from a start node to a goal node in static environments.\n",
    "\n",
    "However, these traditional algorithms face challenges in dynamic or uncertain conditions where factors like randomness, incomplete information, and changing environments come into play. In real-world scenarios, the ability to adapt to such uncertainties is crucial for effective decision-making.\n",
    "\n",
    "Markov Decision Processes (MDPs) offer a robust framework for modeling decision-making in these uncertain settings. By incorporating both rewards and the probabilities of different outcomes, MDPs enable the development of strategies that can adapt and make optimal decisions even when facing variability and unpredictability. This makes MDPs better for creating solutions that navigate complex, real-world environments effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will explore the concept of Markov Decision Processes (MDPs) and how they can be used to model decision-making in a grid-based environment similar to the one used in the pathfinding algorithms. Some new elements such as rewards, transitions are introduced to the environment and will be explained in detail as we progress.\n",
    "\n",
    "Imagine our agent is navigating a 6x5 grid:\n",
    "\n",
    "- **States:** Each cell in the grid represents a state. Some cells are walls (impassable), some are terminal states (once in it, the agent cannot move) with rewards (+1 or -1), and the rest have a small negative reward (-0.04) to encourage the agent to reach a terminal state quickly.\n",
    "- **Actions:** The agent can move **Up**, **Down**, **Left**, or **Right**. However, movement is stochastic:\n",
    "    - 80% chance to move in the intended direction.\n",
    "    - 10% chance to veer to the left of the intended direction.\n",
    "    - 10% chance to veer to the right of the intended direction.\n",
    "- **Rewards:** Entering a terminal state gives a reward of +1 or -1. Moving to any other state incurs a small penalty of -0.04."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States: {(1, 2), (2, 1), (3, 1), (4, 3), (1, 1), (2, 3), (3, 3), (3, 2), (1, 3)}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "\n",
    "class World:\n",
    "    def __init__(self, grid_width, grid_height):\n",
    "        \"\"\"\n",
    "        Initialize the World environment.\n",
    "\n",
    "        Args:\n",
    "            grid_width (int): Width of the grid.\n",
    "            grid_height (int): Height of the grid.\n",
    "        \"\"\"\n",
    "        self.grid_width = grid_width\n",
    "        self.grid_height = grid_height\n",
    "        self.walls = self._initialize_walls()\n",
    "        self.terminal_nodes = {(4, 1): 1, (4, 2): -1}\n",
    "        self.default_reward = -0.04\n",
    "        self.states = self._initialize_states()\n",
    "        self.rewards = self._initialize_rewards()\n",
    "        self.actions = ['up', 'down', 'left', 'right']\n",
    "        self.transition_probs = self._initialize_transition_probs()\n",
    "\n",
    "    def _initialize_walls(self):\n",
    "        \"\"\"\n",
    "        Initialize wall positions in the grid.\n",
    "\n",
    "        Returns:\n",
    "            set: Set of wall coordinates.\n",
    "        \"\"\"\n",
    "        walls = set()\n",
    "        for x in range(self.grid_width):\n",
    "            walls.add((x, 0))\n",
    "            walls.add((x, self.grid_height - 1))\n",
    "        for y in range(self.grid_height):\n",
    "            walls.add((0, y))\n",
    "            walls.add((self.grid_width - 1, y))\n",
    "        walls.add((2, 2))\n",
    "        return walls\n",
    "\n",
    "    def _initialize_states(self):\n",
    "        \"\"\"\n",
    "        Initialize non-wall, non-terminal states.\n",
    "\n",
    "        Returns:\n",
    "            set: Set of state coordinates.\n",
    "        \"\"\"\n",
    "        states = set()\n",
    "        for x in range(self.grid_width):\n",
    "            for y in range(self.grid_height):\n",
    "                if (x, y) not in self.walls:\n",
    "                    states.add((x, y))\n",
    "        states -= set(self.terminal_nodes.keys())\n",
    "        print(f'States: {states}')\n",
    "        return states\n",
    "\n",
    "    def _initialize_rewards(self):\n",
    "        \"\"\"\n",
    "        Initialize rewards for each state.\n",
    "\n",
    "        Returns:\n",
    "            dict: Mapping of state coordinates to rewards.\n",
    "        \"\"\"\n",
    "        rewards = {}\n",
    "        for state in self.states:\n",
    "            rewards[state] = self.default_reward\n",
    "        for state, reward in self.terminal_nodes.items():\n",
    "            rewards[state] = reward\n",
    "        return rewards\n",
    "\n",
    "    def _initialize_transition_probs(self):\n",
    "        \"\"\"\n",
    "        Initialize transition probabilities for each state and action.\n",
    "\n",
    "        Returns:\n",
    "            dict: Nested dictionary of transition probabilities.\n",
    "        \"\"\"\n",
    "        transition_probs = {}\n",
    "        for state in self.states | set(self.terminal_nodes.keys()):\n",
    "            transition_probs[state] = {}\n",
    "            for action in self.actions:\n",
    "                transition_probs[state][action] = self._calculate_move_probabilities(state, action)\n",
    "        return transition_probs\n",
    "\n",
    "    def _calculate_move_probabilities(self, state, action):\n",
    "        \"\"\"\n",
    "        Calculate move probabilities based on action.\n",
    "\n",
    "        Args:\n",
    "            state (tuple): Current state coordinates.\n",
    "            action (str): Action to take.\n",
    "\n",
    "        Returns:\n",
    "            dict: Mapping of next states to probabilities.\n",
    "        \"\"\"\n",
    "        dxdy = {'up': (0, -1), 'down': (0, 1), 'left': (-1, 0), 'right': (1, 0)}\n",
    "        left_right = {'up': ('left', 'right'), 'down': ('right', 'left'), 'left': ('down', 'up'), 'right': ('up', 'down')}\n",
    "        intended = dxdy[action]\n",
    "        left = dxdy[left_right[action][0]]\n",
    "        right = dxdy[left_right[action][1]]\n",
    "\n",
    "        moves = {\n",
    "            intended: 0.8,\n",
    "            left: 0.1,\n",
    "            right: 0.1\n",
    "        }\n",
    "\n",
    "        probabilities = {}\n",
    "        for move, prob in moves.items():\n",
    "            new_state = (state[0] + move[0], state[1] + move[1])\n",
    "            # If the new state is a wall, stay in the same state\n",
    "            if new_state in self.states or new_state in self.terminal_nodes:\n",
    "                probabilities[new_state] = probabilities.get(new_state, 0) + prob\n",
    "            else:\n",
    "                probabilities[state] = probabilities.get(state, 0) + prob\n",
    "\n",
    "        return probabilities\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take a step in the environment based on the action.\n",
    "\n",
    "        Args:\n",
    "            action (str): Action to perform.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Next state and reward.\n",
    "        \"\"\"\n",
    "        if action not in self.actions:\n",
    "            raise ValueError(\"Invalid action\")\n",
    "        probs = self.transition_probs[self.current_state][action]\n",
    "        next_state = random.choices(list(probs.keys()), weights=probs.values())[0]\n",
    "        reward = self.rewards.get(next_state, 0)\n",
    "        self.current_state = next_state\n",
    "        return next_state, reward\n",
    "\n",
    "    def display(self, path=None, display_rewards=True, policy=None):\n",
    "        \"\"\"\n",
    "        Display the grid world.\n",
    "\n",
    "        Args:\n",
    "            path (list, optional): List of state coordinates representing a path.\n",
    "            display_rewards (bool, optional): Whether to display rewards.\n",
    "            policy (dict, optional): Policy mapping states to actions.\n",
    "        \"\"\"\n",
    "        grid = np.zeros((self.grid_height, self.grid_width))\n",
    "\n",
    "        for x in range(self.grid_width):\n",
    "            for y in range(self.grid_height):\n",
    "                if (x, y) in self.walls:\n",
    "                    grid[y, x] = 0\n",
    "                elif (x, y) in self.terminal_nodes:\n",
    "                    grid[y, x] = 1\n",
    "                else:\n",
    "                    grid[y, x] = 2\n",
    "\n",
    "        cmap = ListedColormap(['black', 'green', 'white'])\n",
    "        bounds = [0, 1, 2, 3]\n",
    "        norm = BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        plt.imshow(grid, cmap=cmap, norm=norm)\n",
    "\n",
    "        if display_rewards:\n",
    "            for state, reward in self.rewards.items():\n",
    "                x, y = state\n",
    "                plt.text(x, y, f'{reward:+.2f}', ha='center', va='center', color='blue')\n",
    "\n",
    "        if policy:\n",
    "            arrows = {'up': '\\u2191', 'down': '\\u2193', 'left': '\\u2190', 'right': '\\u2192'}\n",
    "            for state, action in policy.items():\n",
    "                x, y = state\n",
    "                plt.text(x, y, arrows[action], ha='center', va='center', color='red')\n",
    "\n",
    "        if path:\n",
    "            path_x, path_y = zip(*path)\n",
    "            plt.plot(path_x, path_y, marker='o', color='red')\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    def set_rewards(self, new_rewards):\n",
    "        \"\"\"\n",
    "        Update rewards for states.\n",
    "\n",
    "        Args:\n",
    "            new_rewards (dict): Mapping of state coordinates to new rewards.\n",
    "        \"\"\"\n",
    "        self.rewards.update(new_rewards)\n",
    "\n",
    "    def get_transitions(self, state, action):\n",
    "        \"\"\"\n",
    "        Retrieve transitions for a given state and action.\n",
    "\n",
    "        Args:\n",
    "            state (tuple): The current state.\n",
    "            action (str): The action to evaluate.\n",
    "\n",
    "        Returns:\n",
    "            list: List of tuples (probability, next_state, reward).\n",
    "        \"\"\"\n",
    "        transitions = []\n",
    "        for next_state, prob in self.transition_probs[state][action].items():\n",
    "            reward = self.rewards.get(next_state, self.default_reward)\n",
    "            transitions.append((prob, next_state, reward))\n",
    "        return transitions\n",
    "\n",
    "world_env = World(grid_width=6, grid_height=5)\n",
    "world_env.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov Decision Processes (MDPs) provide a mathematical framework for modeling decision-making in environments where outcomes are partly random and partly under the control of a decision maker. \n",
    "\n",
    "### Components of an MDP\n",
    "\n",
    "An MDP is defined by the following components:\n",
    "\n",
    "1. **States (S):** A finite set of states representing all possible configurations of the environment. In our grid-based environment, each cell corresponds to a unique state.\n",
    "\n",
    "2. **Actions (A):** A set of actions available to the agent. In our setup, the agent can perform actions such as moving up, down, left, or right.\n",
    "\n",
    "3. **Transition Probabilities (P):** The probability of transitioning from one state to another given a particular action. Due to the stochastic nature of the environment, actions may not always lead to the intended state. For example, there is a 10% chance the agent moves left or right relative to the intended direction.\n",
    "\n",
    "4. **Rewards (R):** A reward function that assigns a numerical value to each state (or state-action pair). Rewards indicate the immediate benefit or cost of entering a particular state. In our environment, each cell has an associated reward that the agent receives upon entering it.\n",
    "\n",
    "5. **Policy (π):** A strategy that specifies the action the agent should take in each state to maximize cumulative rewards over time. It is basically a lookup table that takes the current state as input and returns the action to be taken.\n",
    "\n",
    "### Objective\n",
    "\n",
    "The primary objective in an MDP is to determine an optimal policy that maximizes the expected sum of rewards the agent receives over time. This involves balancing immediate rewards with future rewards to achieve long-term success.\n",
    "\n",
    "### Solving MDPs\n",
    "\n",
    "In this notebook, we will explore various algorithms for solving MDPs and deriving optimal policies. These algorithms include:\n",
    "\n",
    "- **Value Iteration:** Iteratively updates the value of each state until convergence, then derives the optimal policy based on these values.\n",
    "\n",
    "- **Policy Iteration:** Alternates between evaluating the current policy and improving it until it becomes optimal.\n",
    "\n",
    "- **Q-Learning:** A model-free reinforcement learning algorithm that learns the value of actions directly without needing a model of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value Iteration is a fundamental algorithm used to determine the optimal policy for an agent aiming to maximize its cumulative rewards in a MDP. It systematically updates the value of each state by considering the expected rewards and transition probabilities, enabling the agent to make informed decisions that lead to the best long-term outcomes.\n",
    "\n",
    "### Bellman's Equation\n",
    "\n",
    "Bellman's equation provides a recursive relationship for updating the value of a state based on the values of its neighboring states until convergence. \n",
    "\n",
    "$$\n",
    "V(s) = \\max_{a \\in A} \\left[ R(s) + \\gamma \\sum_{s'} P(s'|s,a) V(s') \\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ V(s) $ is the value of state $ s $.\n",
    "- $ A $ is the set of possible actions.\n",
    "- $ R(s) $ is the immediate reward for being in state $ s $.\n",
    "- $ \\gamma $ is the discount factor. This factor determines the importance of future rewards relative to immediate rewards. A higher discount factor values future rewards more highly.\n",
    "- $ P(s'|s,a) $ is the probability of transitioning to state $ s' $ from state $ s $ when action $ a $ is taken.\n",
    "\n",
    "### Applying Value Iteration to our Grid Environment\n",
    "\n",
    "In our grid-based environment, each cell represents a state, and the agent can perform actions such as **Up**, **Down**, **Left**, or **Right** with stochastic outcomes.\n",
    "\n",
    "For testing, we are using a discount factor ($\\gamma$) of 0.99 and a convergence threshold ($\\epsilon$) of 0.01.\n",
    "\n",
    "Here’s how Value Iteration is applied in this context:\n",
    "\n",
    "1. **Initialize State Values:**\n",
    "    - Assign an initial value (e.g., 0) to all states in `world_env.states`.\n",
    "\n",
    "2. **Iterative Update:**\n",
    "    - For each state $ s $ in `world_env.state`, compute the value $ V(s) $ using Bellman's equation by considering all possible actions and their probabilistic outcomes as defined in `world_env.transition_probs`.\n",
    "\n",
    "3. **Convergence:**\n",
    "    - Repeat the update process until the changes in state values are less than a predefined threshold (here, $\\epsilon = 0.01$) indicating the values have stabilized.\n",
    "\n",
    "4. **Derive Optimal Policy:**\n",
    "    - After convergence, determine the best action for each state by selecting the action that maximizes the expected value based on the updated state values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(world, discount_factor=0.9, epsilon=0.01):\n",
    "    \"\"\"\n",
    "    Perform Value Iteration to compute the optimal value function and policy for the given world environment.\n",
    "\n",
    "    Args:\n",
    "        world (World): The MDP environment containing states, actions, transition probabilities, and rewards.\n",
    "        discount_factor (float): The discount factor gamma, representing the importance of future rewards.\n",
    "        theta (float): A threshold for determining the convergence of the value function.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the value function dictionary V and the optimal policy dictionary policy.\n",
    "    \"\"\"\n",
    "    V = {state: 0 for state in world.states} # V is the value function \n",
    "    print(\"States:\", world.states)\n",
    "\n",
    "    # Iterate for each state to update the value function until convergence\n",
    "    while True:\n",
    "        delta = 0  # Initialize the maximum change in the value function \n",
    "        for state in world.states:\n",
    "            if state in world.terminal_nodes:\n",
    "                print(state)\n",
    "                continue  # Skip terminal states as their values do not change since they have no successors\n",
    "            action_values = [] # Initialize the list to store the expected values of all possible actions\n",
    "            for action in world.actions:\n",
    "                # Calculate the expected value for each action\n",
    "                value = sum(\n",
    "                    prob * (reward + discount_factor * V[next_state])\n",
    "                    for prob, next_state, reward in world.get_transitions(state, action)\n",
    "                )\n",
    "                action_values.append(value)\n",
    "            max_value = max(action_values)  # Select the maximum value among all possible actions\n",
    "            delta = max(delta, abs(max_value - V[state]))  # Update delta to track the maximum change\n",
    "            V[state] = max_value  # Update the value of the current state\n",
    "        if delta < epsilon:\n",
    "            # If the maximum change is below the threshold, the algorithm has converged\n",
    "            break\n",
    "\n",
    "    # Derive the optimal policy for each state based on the computed value function\n",
    "    policy = {}\n",
    "    for state in world.states:\n",
    "        if state in world.terminal_nodes:\n",
    "            policy[state] = None  # No action for terminal states\n",
    "            continue\n",
    "        best_action, max_value = None, float('-inf')\n",
    "        for action in world.actions:\n",
    "            # Compute the expected value of taking this action\n",
    "            value = sum(\n",
    "                prob * (reward + discount_factor * V[next_state])\n",
    "                for prob, next_state, reward in world.get_transitions(state, action)\n",
    "            )\n",
    "            if value > max_value:\n",
    "                best_action, max_value = action, value\n",
    "        policy[state] = best_action  # Assign the best action to the current state\n",
    "    return V, policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States: {(1, 2), (2, 1), (3, 1), (4, 3), (1, 1), (2, 3), (3, 3), (3, 2), (1, 3)}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "(4, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Call the value_iteration function\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m value_function, optimal_policy \u001b[38;5;241m=\u001b[39m \u001b[43mvalue_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscount_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Print the results\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimal Value Function:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[16], line 26\u001b[0m, in \u001b[0;36mvalue_iteration\u001b[1;34m(world, discount_factor, epsilon)\u001b[0m\n\u001b[0;32m     23\u001b[0m action_values \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;66;03m# Initialize the list to store the expected values of all possible actions\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m world\u001b[38;5;241m.\u001b[39mactions:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Calculate the expected value for each action\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprob\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mreward\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdiscount_factor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mworld\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_transitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m     action_values\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[0;32m     31\u001b[0m max_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(action_values)  \u001b[38;5;66;03m# Select the maximum value among all possible actions\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[16], line 27\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     23\u001b[0m action_values \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;66;03m# Initialize the list to store the expected values of all possible actions\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m world\u001b[38;5;241m.\u001b[39mactions:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Calculate the expected value for each action\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\n\u001b[1;32m---> 27\u001b[0m         prob \u001b[38;5;241m*\u001b[39m (reward \u001b[38;5;241m+\u001b[39m discount_factor \u001b[38;5;241m*\u001b[39m \u001b[43mV\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m prob, next_state, reward \u001b[38;5;129;01min\u001b[39;00m world\u001b[38;5;241m.\u001b[39mget_transitions(state, action)\n\u001b[0;32m     29\u001b[0m     )\n\u001b[0;32m     30\u001b[0m     action_values\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[0;32m     31\u001b[0m max_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(action_values)  \u001b[38;5;66;03m# Select the maximum value among all possible actions\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: (4, 1)"
     ]
    }
   ],
   "source": [
    "# Call the value_iteration function\n",
    "value_function, optimal_policy = value_iteration(world_env, discount_factor=0.9, epsilon=0.01)\n",
    "\n",
    "# Print the results\n",
    "print(\"Optimal Value Function:\")\n",
    "for state, value in value_function.items():\n",
    "    print(f\"State {state}: {value}\")\n",
    "\n",
    "print(\"\\nOptimal Policy:\")\n",
    "for state, action in optimal_policy.items():\n",
    "    print(f\"State {state}: {action}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
